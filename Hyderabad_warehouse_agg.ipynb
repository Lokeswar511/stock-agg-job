{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6245090c-7ce9-4199-9349-961a7f16ca52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import date, timedelta, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3267f9df-c93c-4150-ac86-a62f6f65f118",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today is:  2023-10-24\nYesterday was:  2023-10-23\n2023/10/24\n"
     ]
    }
   ],
   "source": [
    "# now = datetime.now()\n",
    "# print(\"now =\", now)\n",
    "# dt_string = now.strftime(\"%Y-%m-%d\")\n",
    "# print(dt_string)\n",
    "\n",
    "# Get today's date\n",
    "today = date.today()\n",
    "print(\"Today is: \", today)\n",
    " \n",
    "# Yesterday date\n",
    "yesterday = today - timedelta(days = 1)\n",
    "print(\"Yesterday was: \", yesterday)\n",
    "\n",
    "dt_string = today.strftime(\"%Y/%m/%d\")\n",
    "print(dt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "563714f9-a594-4fc3-9c78-639c12716874",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "folder = 'Hyderabad'\n",
    "a = f\"/mnt/stock-agg-silver/{folder}/{dt_string}\".format(folder,dt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac0f6903-64cb-4198-8055-cadeafcb5656",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema_def = StructType([StructField('id', IntegerType(), True),\n",
    "                         StructField('StockDate', DateType(), True),\n",
    "                         StructField('WarehouseID', StringType(), True),\n",
    "                         StructField('ItemName', StringType(), True),\n",
    "                         StructField('OpeningStock', IntegerType(), True),\n",
    "                         StructField('Receipts', IntegerType(), True),\n",
    "                         StructField('Issues', IntegerType(), True),\n",
    "                         StructField('UnitValue', FloatType(), True)])\n",
    "\n",
    "Hyderabad_df = spark.read.schema(schema_def).option(\"header\", True).option('inferSchema', True).format('csv').load(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2979ef01-7c0a-4405-9b4f-9da364eef937",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Records available : \n+--------+\n|count(1)|\n+--------+\n|      17|\n+--------+\n\nGlobal Stock Summary: \n+----------+----------------+---------+-------------+--------+------+-------------+-------------+\n| StockDate|        ItemName|TOTAL_REC|OPENING_STOCK|RECEIPTS|ISSUES|CLOSING_STOCK|CLOSING_VALUE|\n+----------+----------------+---------+-------------+--------+------+-------------+-------------+\n|2023-10-24|Labeling Machine|        2|          164|      56|   149|           71|       632.85|\n|2023-10-24|      Calculator|        2|          134|       7|   163|          -22|       -258.4|\n|2023-10-24|        Scissors|        2|           71|      71|    65|           77|      1090.21|\n|2023-10-24|        Notebook|        2|           67|      21|   127|          -39|       -182.0|\n|2023-10-24|    Sticky Notes|        2|           12|      41|    11|           42|       333.16|\n|2023-10-24|  Tape Dispenser|        2|          114|      30|    68|           76|       814.43|\n|2023-10-24|       Clipboard|        1|           83|      29|    37|           75|       794.25|\n|2023-10-24|      Pencil Box|        1|           83|      22|    40|           65|       356.85|\n|2023-10-24|Pencil Sharpener|        2|          157|      75|   167|           65|       930.79|\n|2023-10-24|         Folders|        1|           48|      42|    83|            7|       119.98|\n+----------+----------------+---------+-------------+--------+------+-------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Create a temporary view\n",
    "Hyderabad_df.createOrReplaceTempView(\"GLOBAL_STOCK\")\n",
    "\n",
    "print(\"Total Records available : \")\n",
    "spark.sql(\"SELECT count(*) FROM GLOBAL_STOCK\").show()\n",
    "\n",
    "# Perform the aggregation using DataFrame API\n",
    "Hyderabad_stockSummary = Hyderabad_df.groupBy(\"StockDate\", \"ItemName\").agg(\n",
    "    count(\"*\").alias(\"TOTAL_REC\"),\n",
    "    sum(\"OpeningStock\").alias(\"OPENING_STOCK\"),\n",
    "    sum(\"Receipts\").alias(\"RECEIPTS\"),\n",
    "    sum(\"Issues\").alias(\"ISSUES\"),\n",
    "    sum((col(\"OpeningStock\") + col(\"Receipts\") - col(\"Issues\"))).alias(\"CLOSING_STOCK\"),\n",
    "    sum((col(\"OpeningStock\") + col(\"Receipts\") - col(\"Issues\")) * col(\"UnitValue\")).alias(\"CLOSING_VALUE\")\n",
    ")\n",
    "\n",
    "print(\"Global Stock Summary: \")\n",
    "daily_Hyderabad_agg = Hyderabad_stockSummary.withColumn('CLOSING_VALUE', round(col('CLOSING_VALUE'),2))\n",
    "\n",
    "daily_Hyderabad_agg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7890ff39-6a82-4b3e-8995-9ccdef7de529",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10\n24\n/mnt/stock-agg-gold/Hyderabad/2023-10/24/Hyderabad_data_.csv\n"
     ]
    }
   ],
   "source": [
    "#writing to Gold\n",
    "\n",
    "year_month = today.strftime(\"%Y-%m\")\n",
    "print(year_month)\n",
    "day = today.strftime(\"%d\")\n",
    "print(day)\n",
    "output_path = '/mnt/stock-agg-gold/Hyderabad/{0}/{1}/Hyderabad_data_.csv'.format(year_month, day)\n",
    "print(output_path)\n",
    "\n",
    "daily_Hyderabad_agg.write.mode('overwrite').format('csv').save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e300792d-123b-4040-a0dc-5a0e19a36c9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Hyderabad_warehouse_agg",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
